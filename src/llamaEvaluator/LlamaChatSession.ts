import {DisposeAggregator, DisposedError, EventRelay, withLock} from "lifecycle-utils";
import {defaultChatSystemPrompt} from "../config.js";
import {ChatPromptWrapper} from "../ChatPromptWrapper.js";
import {AbortError} from "../AbortError.js";
import {GeneralChatPromptWrapper} from "../chatWrappers/GeneralChatPromptWrapper.js";
import {resolveChatWrapperBasedOnModel} from "../chatWrappers/resolveChatWrapperBasedOnModel.js";
import {ConversationInteraction, Token} from "../types.js";
import {generateContextTextFromConversationHistory} from "../chatWrappers/generateContextTextFromConversationHistory.js";
import {removeNullFields} from "../utils/removeNullFields.js";
import {LlamaModel} from "./LlamaModel.js";
import {LlamaContextSequence} from "./LlamaContext/LlamaContext.js";
import {LlamaGrammar} from "./LlamaGrammar.js";
import {LlamaGrammarEvaluationState} from "./LlamaGrammarEvaluationState.js";

const UNKNOWN_UNICODE_CHAR = "\ufffd";


export type LlamaChatSessionOptions = {
    contextSequence: LlamaContextSequence,
    printLLamaSystemInfo?: boolean,

    /** `"auto"` is used by default */
    promptWrapper?: "auto" | ChatPromptWrapper,

    systemPrompt?: string,

    /** Conversation history to load into the context to continue an existing conversation */
    conversationHistory?: readonly ConversationInteraction[]

    /** Automatically dispose the sequence when the session is disposed */
    autoDisposeSequence?: boolean
};

export type LLamaChatPromptOptions = {
    onToken?: (tokens: Token[]) => void,
    signal?: AbortSignal,
    maxTokens?: number,

    /**
     * Temperature is a hyperparameter that controls the randomness of the generated text.
     * It affects the probability distribution of the model's output tokens.
     * A higher temperature (e.g., 1.5) makes the output more random and creative,
     * while a lower temperature (e.g., 0.5) makes the output more focused, deterministic, and conservative.
     * The suggested temperature is 0.8, which provides a balance between randomness and determinism.
     * At the extreme, a temperature of 0 will always pick the most likely next token, leading to identical outputs in each run.
     *
     * Set to `0` to disable.
     * Disabled by default (set to `0`).
     */
    temperature?: number,

    /**
     * Limits the model to consider only the K most likely next tokens for sampling at each step of sequence generation.
     * An integer number between `1` and the size of the vocabulary.
     * Set to `0` to disable (which uses the full vocabulary).
     *
     * Only relevant when `temperature` is set to a value greater than 0.
     */
    topK?: number,

    /**
     * Dynamically selects the smallest set of tokens whose cumulative probability exceeds the threshold P,
     * and samples the next token only from this set.
     * A float number between `0` and `1`.
     * Set to `1` to disable.
     *
     * Only relevant when `temperature` is set to a value greater than `0`.
     */
    topP?: number,

    grammar?: LlamaGrammar,

    /**
     * Trim whitespace from the end of the generated text
     * Disabled by default.
     */
    trimWhitespaceSuffix?: boolean,

    repeatPenalty?: false | LlamaChatSessionRepeatPenalty
};

export type LlamaChatSessionRepeatPenalty = {
    /**
     * Number of recent tokens generated by the model to apply penalties to repetition of.
     * Defaults to `64`.
     */
    lastTokens?: number,

    punishTokensFilter?: (tokens: Token[]) => Token[],

    /**
     * Penalize new line tokens.
     * Enabled by default.
     */
    penalizeNewLine?: boolean,

    /**
     * The relative amount to lower the probability of the tokens in `punishTokens` by
     * Defaults to `1.1`.
     * Set to `1` to disable.
     */
    penalty?: number,

    /**
     * For n time a token is in the `punishTokens` array, lower its probability by `n * frequencyPenalty`
     * Disabled by default (`0`).
     * Set to a value between `0` and `1` to enable.
     */
    frequencyPenalty?: number,

    /**
     * Lower the probability of all the tokens in the `punishTokens` array by `presencePenalty`
     * Disabled by default (`0`).
     * Set to a value between `0` and `1` to enable.
     */
    presencePenalty?: number
};

export class LlamaChatSession {
    /** @internal */ private readonly _systemPrompt: string;
    /** @internal */ private readonly _printLLamaSystemInfo: boolean;
    /** @internal */ private readonly _promptWrapper: ChatPromptWrapper;
    /** @internal */ private readonly _disposeAggregator = new DisposeAggregator();
    /** @internal */ private readonly _autoDisposeSequence: boolean;
    /** @internal */ private _promptIndex: number = 0;
    /** @internal */ private _initialized: boolean = false;
    /** @internal */ private _lastStopString: string | null = null;
    /** @internal */ private _lastStopStringSuffix: string | null = null;
    /** @internal */ private _conversationHistoryToLoad: readonly ConversationInteraction[] | null = null;
    /** @internal */ private _sequence: LlamaContextSequence | null;

    public readonly onDispose = new EventRelay<void>();

    /**
     * @param options
     */
    public constructor({
        contextSequence,
        printLLamaSystemInfo = false,
        promptWrapper = "auto",
        systemPrompt = defaultChatSystemPrompt,
        conversationHistory,
        autoDisposeSequence = true
    }: LlamaChatSessionOptions) {
        if (contextSequence.disposed)
            throw new DisposedError();

        this._sequence = contextSequence;
        this._printLLamaSystemInfo = printLLamaSystemInfo;
        this._systemPrompt = systemPrompt;
        this._conversationHistoryToLoad = (conversationHistory != null && conversationHistory.length > 0)
            ? conversationHistory
            : null;
        this._autoDisposeSequence = autoDisposeSequence;

        this._disposeAggregator.add(
            this._sequence.onDispose.createListener(() => {
                this.dispose();
            })
        );
        this._disposeAggregator.add(this.onDispose.dispatchEvent);

        if (promptWrapper === "auto") {
            const chatWrapper = resolveChatWrapperBasedOnModel({
                bosString: contextSequence.model.tokens.bosString,
                filename: contextSequence.model.filename,
                typeDescription: contextSequence.model.typeDescription
            });

            if (chatWrapper != null)
                this._promptWrapper = new chatWrapper();
            else
                this._promptWrapper = new GeneralChatPromptWrapper();
        } else
            this._promptWrapper = promptWrapper;
    }

    public dispose({disposedSequence = this._autoDisposeSequence}: {disposedSequence?: boolean} = {}) {
        if (this._sequence == null)
            return;

        if (disposedSequence)
            this._sequence.dispose();

        this._sequence = null;

        this._disposeAggregator.dispose();
    }

    /** @hidden */
    public [Symbol.dispose]() {
        return this.dispose();
    }

    public get disposed() {
        return this._sequence == null;
    }

    public get initialized() {
        return this._initialized;
    }

    public get sequence() {
        if (this._sequence == null)
            throw new DisposedError();

        return this._sequence;
    }

    public async init() {
        await withLock(this, "init", async () => {
            this._ensureNotDisposed();

            if (this._initialized)
                return;

            if (this._printLLamaSystemInfo)
                console.log("Llama system info", LlamaModel.systemInfo);

            this._initialized = true;
        });
    }

    /**
     * @param prompt
     * @param [options]
     */
    public async prompt(prompt: string, {
        onToken,
        signal,
        maxTokens,
        temperature,
        topK,
        topP,
        grammar,
        trimWhitespaceSuffix = false,
        repeatPenalty
    }: LLamaChatPromptOptions = {}) {
        const {text} = await this.promptWithMeta(prompt, {
            onToken, signal, maxTokens, temperature, topK, topP, grammar, trimWhitespaceSuffix, repeatPenalty
        });

        return text;
    }

    /**
     * @param prompt
     * @param [options]
     */
    public async promptWithMeta(prompt: string, {
        onToken,
        signal,
        maxTokens,
        temperature,
        topK,
        topP,
        grammar,
        trimWhitespaceSuffix = false,
        repeatPenalty
    }: LLamaChatPromptOptions = {}) {
        this._ensureNotDisposed();

        if (!this.initialized)
            await this.init();

        return await withLock(this, "prompt", async () => {
            this._ensureNotDisposed();

            if (this._sequence == null)
                throw new DisposedError();

            let promptText = "";

            if (this._promptIndex == 0 && this._conversationHistoryToLoad != null) {
                const {text, stopString, stopStringSuffix} =
                    generateContextTextFromConversationHistory(this._promptWrapper, this._conversationHistoryToLoad, {
                        systemPrompt: this._systemPrompt,
                        currentPromptIndex: this._promptIndex,
                        lastStopString: this._lastStopString,
                        lastStopStringSuffix: this._promptIndex == 0
                            ? (
                                this._sequence.prependBos
                                    ? this._sequence.context.model.tokens.bosString
                                    : null
                            )
                            : this._lastStopStringSuffix
                    });

                promptText += text;
                this._lastStopString = stopString;
                this._lastStopStringSuffix = stopStringSuffix;
                this._promptIndex += this._conversationHistoryToLoad.length;

                this._conversationHistoryToLoad = null;
            }

            promptText += this._promptWrapper.wrapPrompt(prompt, {
                systemPrompt: this._systemPrompt,
                promptIndex: this._promptIndex,
                lastStopString: this._lastStopString,
                lastStopStringSuffix: this._promptIndex == 0
                    ? (
                        this._sequence.prependBos
                            ? this._sequence.context.model.tokens.bosString
                            : null
                    )
                    : this._lastStopStringSuffix
            });
            this._promptIndex++;
            this._lastStopString = null;
            this._lastStopStringSuffix = null;

            const {text, stopReason, stopString, stopStringSuffix} =
                await this._evalTokens(this._sequence.context.model.tokenize(promptText), {
                    onToken, signal, maxTokens, temperature, topK, topP, grammar, trimWhitespaceSuffix,
                    repeatPenalty: repeatPenalty == false ? {lastTokens: 0} : repeatPenalty
                });
            this._lastStopString = stopString;
            this._lastStopStringSuffix = stopStringSuffix;

            return {
                text,
                stopReason,
                stopString,
                stopStringSuffix
            };
        });
    }

    /** @internal */
    private async _evalTokens(tokens: Token[], {
        onToken,
        signal,
        maxTokens,
        temperature,
        topK,
        topP,
        grammar,
        trimWhitespaceSuffix = false,
        repeatPenalty: {
            lastTokens: repeatPenaltyLastTokens = 64,
            punishTokensFilter,
            penalizeNewLine,
            penalty,
            frequencyPenalty,
            presencePenalty
        } = {}
    }: {
        onToken?: (tokens: Token[]) => void,
        signal?: AbortSignal,
        maxTokens?: number,
        temperature?: number,
        topK?: number,
        topP?: number,
        grammar?: LlamaGrammar,
        trimWhitespaceSuffix?: boolean,
        repeatPenalty?: LlamaChatSessionRepeatPenalty
    } = {}) {
        if (this._sequence == null)
            throw new DisposedError();

        let stopStrings = this._promptWrapper.getStopStrings();

        if (grammar != null)
            stopStrings = stopStrings.concat(grammar.stopStrings);

        const stopStringIndexes: number[] = Array(stopStrings.length).fill(0);
        const skippedChunksQueue: Token[] = [];
        const res: Token[] = [];
        const grammarEvaluationState = grammar != null
            ? new LlamaGrammarEvaluationState({grammar})
            : undefined;
        const repeatPenaltyEnabled = repeatPenaltyLastTokens > 0;
        let stopReason: "eosToken" | "stopString" | "maxTokens" = "eosToken";

        const getPenaltyTokens = () => {
            let punishTokens = res.slice(-repeatPenaltyLastTokens);

            if (punishTokensFilter != null)
                punishTokens = punishTokensFilter(punishTokens);

            if (!penalizeNewLine) {
                const nlToken = this.sequence.context.model.tokens.nl;

                if (nlToken != null)
                    punishTokens = punishTokens.filter(token => token !== nlToken);
            }

            return Uint32Array.from(punishTokens);
        };

        const evaluationIterator = this._sequence.evaluate(tokens, removeNullFields({
            temperature, topK, topP, grammarEvaluationState,
            repeatPenalty: !repeatPenaltyEnabled ? undefined : {
                punishTokens: getPenaltyTokens,
                penalty,
                frequencyPenalty,
                presencePenalty
            }
        }));

        for await (const chunk of evaluationIterator) {
            if (signal?.aborted)
                throw new AbortError();

            if (this._sequence == null)
                throw new DisposedError();

            const tokenStr = this._sequence.context.model.detokenize([chunk]);
            const {
                shouldReturn, skipTokenEvent, stopString, stopStringSuffix
            } = this._checkStopString(tokenStr, stopStrings, stopStringIndexes);

            if (shouldReturn) {
                skippedChunksQueue.push(chunk);
                const skippedChunksText = skippedChunksQueue.length > 0
                    ? this._sequence.context.model.detokenize(skippedChunksQueue)
                    : "";

                let [queuedTextBeforeStopString] = skippedChunksText.split(stopString);

                if (grammar?.trimWhitespaceSuffix || trimWhitespaceSuffix)
                    queuedTextBeforeStopString = queuedTextBeforeStopString.trimEnd();

                if (queuedTextBeforeStopString.length > 0) {
                    const beforeStopStringTokens: Token[] = Array.from(this._sequence.context.model.tokenize(queuedTextBeforeStopString));

                    res.push(...beforeStopStringTokens);
                    onToken?.(beforeStopStringTokens);
                    skippedChunksQueue.length = 0;
                }

                stopReason = "stopString";

                return {
                    text: this._sequence.context.model.detokenize(res),
                    stopReason,
                    stopString,
                    stopStringSuffix
                };
            }

            // if the token is unknown, it means it's not complete character
            if (tokenStr === UNKNOWN_UNICODE_CHAR || skipTokenEvent || (
                (grammar?.trimWhitespaceSuffix || trimWhitespaceSuffix) && tokenStr.trim() === ""
            )) {
                skippedChunksQueue.push(chunk);
                continue;
            }

            if (skippedChunksQueue.length > 0) {
                res.push(...skippedChunksQueue);
                onToken?.(skippedChunksQueue);
                skippedChunksQueue.length = 0;
            }

            res.push(chunk);
            onToken?.([chunk]);

            if (maxTokens != null && maxTokens > 0 && res.length >= maxTokens) {
                stopReason = "maxTokens";
                break;
            }
        }

        if (this._sequence == null)
            throw new DisposedError();

        let resText = this._sequence.context.model.detokenize(res);

        if (grammar?.trimWhitespaceSuffix || trimWhitespaceSuffix)
            resText = resText.trimEnd();

        return {
            text: resText,
            stopReason,
            stopString: null,
            stopStringSuffix: null
        };
    }

    /** @internal */
    private _checkStopString(tokenStr: string, stopStrings: string[], stopStringIndexes: number[]){
        let skipTokenEvent = false;

        for (let stopStringIndex = 0; stopStringIndex < stopStrings.length; stopStringIndex++) {
            const stopString = stopStrings[stopStringIndex];

            let localShouldSkipTokenEvent = false;
            let i = 0;
            for (; i < tokenStr.length && stopStringIndexes[stopStringIndex] !== stopString.length; i++) {
                if (tokenStr[i] === stopString[stopStringIndexes[stopStringIndex]]) {
                    stopStringIndexes[stopStringIndex]++;
                    localShouldSkipTokenEvent = true;
                } else {
                    stopStringIndexes[stopStringIndex] = 0;
                    localShouldSkipTokenEvent = false;
                }
            }

            if (stopStringIndexes[stopStringIndex] === stopString.length) {
                return {
                    shouldReturn: true,
                    stopString,
                    stopStringSuffix: tokenStr.length === i
                        ? null
                        : tokenStr.slice(i)
                };
            }

            skipTokenEvent ||= localShouldSkipTokenEvent;
        }

        return {skipTokenEvent};
    }

    /** @internal */
    private _ensureNotDisposed() {
        if (this._sequence == null)
            throw new DisposedError();
    }
}
